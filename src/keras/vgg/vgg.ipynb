{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout,Flatten, Dense\n",
    "from keras import applications\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tweakable\n",
    "epochs = 1\n",
    "batch_size = 16\n",
    "\n",
    "#Constants\n",
    "target_img_width, target_img_height = 150,150\n",
    "train_dir = '../../../dataset/split_data/train/'\n",
    "val_dir = '../../../dataset/split_data/validation/'\n",
    "train_features_file = \"train_features.npy\"\n",
    "val_features_file = \"val_features.npy\"\n",
    "num_classes = 10\n",
    "top_model_weights_path = 'top_model_weights.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract feature-vectors from VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features(model, data_dir):\n",
    "    #Create a generator to load the data\n",
    "    datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    generator = datagen.flow_from_directory(data_dir, \n",
    "                                            target_size=(target_img_width, target_img_height),\n",
    "                                            batch_size=batch_size, \n",
    "                                            class_mode=None, #only the data, without labels\n",
    "                                            shuffle=False) #keep data ordered \n",
    "    #Extract information about the data\n",
    "    num_samples = len(generator.filenames)\n",
    "    num_classes = len(generator.class_indices)\n",
    "    \n",
    "    #Obtain number of steps required\n",
    "    steps = ceil(num_samples / batch_size)\n",
    "    #print(\"steps %s\" % steps)\n",
    "    \n",
    "    #Obtain the bottleneck features before the dense layers\n",
    "    features = model.predict_generator(generator, steps=steps, verbose=1)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_vgg16_features():\n",
    "    #Load the VGG16 Model\n",
    "    model = applications.VGG16(include_top=False, weights=\"imagenet\")\n",
    "    \n",
    "    #-----------------TRAINING DATA------------------\n",
    "    #Run the training data through vgg and obtain the corresponding features\n",
    "    train_features = get_features(model, train_dir)                \n",
    "    \n",
    "    #Save the training features in a numpy file\n",
    "    np.save(train_features_file, train_features)\n",
    "    print(\"Saved Training Features in %s\" % train_features_file)\n",
    "    \n",
    "    #-----------------VALIDATION DATA------------------\n",
    "    #Run the training data through vgg and obtain the corresponding features\n",
    "    val_features = get_features(model, val_dir)                \n",
    "    \n",
    "    #Save the validation features in a numpy file\n",
    "    np.save(val_features_file, val_features)\n",
    "    print(\"Saved Validation Features in %s\" % val_features_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the top Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_top_model(final_activation,input_shape):\n",
    "    model = Sequential()  \n",
    "    model.add(Flatten(input_shape=input_shape))  \n",
    "    model.add(Dense(256, activation='relu'))  \n",
    "    model.add(Dropout(0.5))  \n",
    "    model.add(Dense(num_classes, activation=final_activation)) # sigmoid to train, softmax for prediction\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_and_labels(features_file, data_dir):\n",
    "    #Create the datagen\n",
    "    datagen = ImageDataGenerator(rescale=1.0/255.0) \n",
    "        \n",
    "    #Create the generator to load the data\n",
    "    generator = datagen.flow_from_directory(data_dir, \n",
    "                                            target_size=(target_img_width, target_img_height),\n",
    "                                            batch_size=batch_size,\n",
    "                                            class_mode='categorical', #specify categorical\n",
    "                                            shuffle=False #Data is ordered\n",
    "                                           )\n",
    "    #Obtain information about the data\n",
    "    num_samples = len(generator.filenames)\n",
    "    num_classes = len(generator.class_indices)\n",
    "    \n",
    "    #Load the training data features\n",
    "    data = np.load(features_file)\n",
    "    \n",
    "    #Obtain class labels from the generator\n",
    "    labels = generator.classes    \n",
    "    #Convert into onehot \n",
    "    labels_onehot = to_categorical(labels, num_classes=num_classes)\n",
    "    \n",
    "    return data, labels_onehot\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_top_model():\n",
    "    # Load the TRAINING data and labels\n",
    "    train_data, train_labels = load_data_and_labels(train_features_file, train_dir)\n",
    "    \n",
    "    print(train_data.shape, train_labels.shape) #temp\n",
    "    \n",
    "    # Load the VALIDATION data and labels\n",
    "    val_data, val_labels = load_data_and_labels(val_features_file, val_dir)\n",
    "    \n",
    "    print(val_data.shape, val_labels.shape) #temp\n",
    "    \n",
    "    #Create the top model to be trained\n",
    "    model = create_top_model(\"sigmoid\",train_data.shape[1:])\n",
    "    \n",
    "    #Compile the model\n",
    "    model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    #Train the model\n",
    "    model.fit(train_data, \n",
    "              train_labels,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(val_data, val_labels)\n",
    "             )\n",
    "    \n",
    "    #Save the trained weights of the model\n",
    "    model.save_weights(top_model_weights_path)\n",
    "    \n",
    "#     #Obtain a final Accuracy\n",
    "    (loss, accuracy) = model.evaluate(val_data, val_labels, batch_size=batch_size, verbose=1)\n",
    "    \n",
    "#     print(\"------------TOTAL-----------\")\n",
    "    print(\"Final Accuracy =\", accuracy*100, \"%\")\n",
    "    print(\"Final Loss=\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13447 images belonging to 10 classes.\n",
      "(13447, 4, 4, 512) (13447, 10)\n",
      "Found 4487 images belonging to 10 classes.\n",
      "(4487, 4, 4, 512) (4487, 10)\n",
      "Train on 13447 samples, validate on 4487 samples\n",
      "Epoch 1/1\n",
      "13447/13447 [==============================] - 30s 2ms/step - loss: 1.4790 - acc: 0.4695 - val_loss: 0.2726 - val_acc: 0.9151\n",
      "4487/4487 [==============================] - 1s 263us/step\n",
      "Final Accuracy = 91.5088032119 %\n",
      "Final Loss= 0.272639715409\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #extract_vgg16_features() #Takes quite a bit of time on CPU\n",
    "    train_top_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
