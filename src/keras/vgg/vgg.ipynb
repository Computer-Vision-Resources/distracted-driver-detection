{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout,Flatten, Dense\n",
    "from keras import applications\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tweakable\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "\n",
    "#Constants\n",
    "target_img_width, target_img_height = 224, 224\n",
    "train_dir = '../../../dataset/split_data/train/'\n",
    "val_dir = '../../../dataset/split_data/validation/'\n",
    "test_dir = '../../../dataset/split_data/test/'\n",
    "vgg_train_features_file = \"vgg_train_features.npy\"\n",
    "vgg_val_features_file = \"vgg_val_features.npy\"\n",
    "vgg_test_features_file = \"vgg_test_features.npy\"\n",
    "num_classes = 10\n",
    "top_model_weights_path = 'top_model_weights.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract feature-vectors from VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features(model, data_dir):\n",
    "    #Create a generator to load the data\n",
    "    datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    generator = datagen.flow_from_directory(data_dir, \n",
    "                                            target_size=(target_img_width, target_img_height),\n",
    "                                            batch_size=batch_size, \n",
    "                                            class_mode=None, #only the data, without labels\n",
    "                                            shuffle=False) #keep data ordered \n",
    "    #Extract information about the data\n",
    "    num_samples = len(generator.filenames)\n",
    "    num_classes = len(generator.class_indices)\n",
    "    \n",
    "    #Obtain number of steps required\n",
    "    steps = ceil(num_samples / batch_size)\n",
    "    #print(\"steps %s\" % steps)\n",
    "    \n",
    "    #Obtain the bottleneck features before the dense layers\n",
    "    features = model.predict_generator(generator, steps=steps, verbose=1)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_vgg16_features():\n",
    "    #Load the VGG16 Model\n",
    "    model = applications.VGG16(include_top=False, weights=\"imagenet\")\n",
    "    \n",
    "    #-----------------TRAINING DATA------------------\n",
    "    #Run the training data through vgg and obtain the corresponding features\n",
    "    train_features = get_features(model, train_dir)                \n",
    "    \n",
    "    #Save the training features in a numpy file\n",
    "    np.save(vgg_train_features_file, train_features)\n",
    "    print(\"Saved Training Features in %s\" % vgg_train_features_file)\n",
    "    \n",
    "    #-----------------VALIDATION DATA------------------\n",
    "    #Run the validation data through vgg and obtain the corresponding features\n",
    "    val_features = get_features(model, val_dir)                \n",
    "    \n",
    "    #Save the validation features in a numpy file\n",
    "    np.save(vgg_val_features_file, val_features)\n",
    "    print(\"Saved Validation Features in %s\" % vgg_val_features_file)\n",
    "    \n",
    "    \n",
    "    #-----------------TESTING DATA------------------\n",
    "    #Run the testing data through vgg and obtain the corresponding features\n",
    "    test_features = get_features(model, test_dir)                \n",
    "    \n",
    "    #Save the testing features in a numpy file\n",
    "    np.save(vgg_test_features_file, test_features)\n",
    "    print(\"Saved Testing Features in %s\" % vgg_test_features_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Model to be Retrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_top_model(final_activation,input_shape):\n",
    "    \n",
    "    model = Sequential()  \n",
    "    model.add(Flatten(input_shape=input_shape))  \n",
    "    model.add(Dense(256, activation='relu'))  \n",
    "    model.add(Dropout(0.5))  \n",
    "    model.add(Dense(num_classes, activation=final_activation)) # sigmoid to train, softmax for prediction\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_and_labels(features_file, data_dir):\n",
    "    #Create the datagen\n",
    "    datagen = ImageDataGenerator(rescale=1.0/255.0) \n",
    "        \n",
    "    #Create the generator to load the data\n",
    "    generator = datagen.flow_from_directory(data_dir, \n",
    "                                            target_size=(target_img_width, target_img_height),\n",
    "                                            batch_size=batch_size,\n",
    "                                            class_mode='categorical', #specify categorical\n",
    "                                            shuffle=False #Data is ordered\n",
    "                                           )\n",
    "    #Obtain information about the data\n",
    "    num_samples = len(generator.filenames)\n",
    "    num_classes = len(generator.class_indices)\n",
    "    \n",
    "    #Load the training data features\n",
    "    data = np.load(features_file)\n",
    "    \n",
    "    #Obtain class labels from the generator\n",
    "    labels = generator.classes    \n",
    "    #Convert into onehot \n",
    "    labels_onehot = to_categorical(labels, num_classes=num_classes)\n",
    "    \n",
    "    return data, labels_onehot\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_top_model():\n",
    "    \n",
    "    # Load the TRAINING data and labels\n",
    "    train_data, train_labels = load_data_and_labels(vgg_train_features_file, train_dir)    \n",
    "    \n",
    "    # Load the VALIDATION data and labels\n",
    "    val_data, val_labels = load_data_and_labels(vgg_val_features_file, val_dir)    \n",
    "    \n",
    "    #Create the top model to be trained\n",
    "    model = create_top_model(\"sigmoid\", train_data.shape[1:])\n",
    "    \n",
    "    #Compile the model\n",
    "    model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "                            top_model_weights_path,\n",
    "                            monitor='val_acc',\n",
    "                            verbose=1,\n",
    "                            save_best_only=True,\n",
    "                            mode='max')\n",
    "\n",
    "    early_stop_callback = EarlyStopping(\n",
    "                            monitor='val_acc',\n",
    "                            patience=3,\n",
    "                            mode='max') \n",
    "\n",
    "    callbacks_list = [checkpoint_callback, early_stop_callback]\n",
    "    \n",
    "    #Train the model\n",
    "    history = model.fit(\n",
    "                train_data,\n",
    "                train_labels,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                validation_data=(val_data, val_labels),\n",
    "                callbacks=callbacks_list)\n",
    "    \n",
    "    #Save the trained weights of the model\n",
    "    # model.save_weights(top_model_weights_path)    \n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model(model):\n",
    "    \n",
    "    # Load the TESTING data and labels\n",
    "    test_data, test_labels = load_data_and_labels(test_features_file, test_dir) \n",
    "    \n",
    "    #Obtain a final Accuracy\n",
    "    (loss, accuracy) = model.evaluate(test_data, test_labels, batch_size=batch_size, verbose=1)        \n",
    "    \n",
    "    print(\"------------TOTAL-----------\")\n",
    "    print(\"Final Accuracy =\", accuracy*100, \"%\")\n",
    "    print(\"Final Loss=\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_prediction_from_image(img_path):\n",
    "    \n",
    "    class_labels = ['safe_driving', 'texting_right', 'talking_on_phone_right', 'texting_left', 'talking_on_phone_left',\n",
    "                'operating_radio', 'drinking', 'reaching_behind', 'doing_hair_makeup', 'talking_to_passanger']\n",
    "\n",
    "    target_size=(150,150)\n",
    "\n",
    "    # prepare image for classification using keras utility functions\n",
    "    image = load_img(img_path, target_size=target_size)\n",
    "    \n",
    "    # print image\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    image = img_to_array(image) # convert from PIL Image to NumPy array\n",
    "    image /= 255\n",
    "    # the dimensions of image should now be (150, 150, 3)\n",
    "\n",
    "    # to be able to pass it through the network and use batches, we want it with shape (1, 224, 224, 3)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    # print(image.shape)\n",
    "\n",
    "    # build the VGG16 network  \n",
    "    model = applications.VGG16(include_top=False, weights='imagenet')  \n",
    "\n",
    "    # get the bottleneck prediction from the pre-trained VGG16 model  \n",
    "    bottleneck_prediction = model.predict(image) \n",
    "    \n",
    "    # build top model  \n",
    "    model = create_top_model(\"softmax\", bottleneck_prediction.shape[1:])\n",
    "\n",
    "    model.load_weights(top_model_weights_path)  \n",
    "\n",
    "    # use the bottleneck prediction on the top model to get the final classification  \n",
    "    class_predicted = model.predict_classes(bottleneck_prediction) \n",
    "    \n",
    "    probs = model.predict(bottleneck_prediction) \n",
    "    decoded_predictions = dict(zip(class_labels, probs[0]))\n",
    "    decoded_predictions = sorted(decoded_predictions.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    count = 1\n",
    "    for key, value in decoded_predictions[:5]:\n",
    "        print(\"{}. {}: {:8f}%\".format(count, key, value*100))\n",
    "        count+=1\n",
    "\n",
    "    # print(class_predicted)\n",
    "    # print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13447 images belonging to 10 classes.\n",
      "Found 4487 images belonging to 10 classes.\n",
      "Train on 13447 samples, validate on 4487 samples\n",
      "Epoch 1/1\n",
      "13447/13447 [==============================] - 50s 4ms/step - loss: 2.1784 - acc: 0.2511 - val_loss: 0.4734 - val_acc: 0.8696\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_features_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-fead457d549a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# ----------STEP 3-----------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# ----------STEP 4-----------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-a5350e493a6d>\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Load the TESTING data and labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_and_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#Obtain a final Accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_features_file' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # ----------STEP 1-----------\n",
    "    #extract_vgg16_features() #Computationally heavy step\n",
    "    \n",
    "    # ----------STEP 2-----------\n",
    "    model = train_top_model()\n",
    "    \n",
    "    # ----------STEP 3-----------\n",
    "    test_model(model)\n",
    "    \n",
    "    # ----------STEP 4-----------\n",
    "    get_prediction_from_image(\"../../../dataset/split_data/test/c0/img_42043.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
